{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCm4fzlxpb1"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWQl5SV9Qf9I",
        "outputId": "cbef6316-60e6-4e21-f60a-ab48d13ab3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nowar/DualStyleGAN\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/williamyang1991/DualStyleGAN.git\n",
        "%cd DualStyleGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZaKlRr1RB3T",
        "outputId": "f8789eb4-08fb-468d-d132-25fb053cfa6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.35.0-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.35.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install wandb\n",
        "! pip install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044evl18NiqL",
        "outputId": "cd0cb92c-0944-4482-ffc3-c70aef6caf05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ],
      "source": [
        "# !wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metGF5uyQ7bt",
        "outputId": "94537b2b-86fd-4716-d053-299f14bcbca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make dataset of image sizes: 1024\n",
            "170it [00:12, 13.61it/s]\n"
          ]
        }
      ],
      "source": [
        "! python ./model/stylegan/prepare_data.py --out ./data/nowar_soldier/lmdb/ --n_worker 4 --size 1024 ./data/nowar_soldier/images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIub1SqLNjYB"
      },
      "source": [
        "# 1. Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV647X3YRVHE",
        "outputId": "11af27d1-c321-4747-be89-937ccc1acc12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZuiFo-gJVlL"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "import torch.distributed as dist\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "from util import data_sampler, requires_grad, accumulate, sample_data, d_logistic_loss, d_r1_loss, g_nonsaturating_loss, g_path_regularize, make_noise, mixing_noise, set_grad_none"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKA9AUqwNMot"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import wandb\n",
        "\n",
        "except ImportError:\n",
        "    wandb = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8pCJioSC_O"
      },
      "outputs": [],
      "source": [
        "from model.stylegan.dataset import MultiResolutionDataset\n",
        "from model.stylegan.distributed import (\n",
        "    get_rank,\n",
        "    synchronize,\n",
        "    reduce_loss_dict,\n",
        "    reduce_sum,\n",
        "    get_world_size,\n",
        ")\n",
        "from model.stylegan.non_leaking import augment, AdaptiveAugment\n",
        "from model.stylegan.model import Generator, Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzWBS_AISe4B"
      },
      "outputs": [],
      "source": [
        "# soldier style\n",
        "style = 'nowar_soldier'\n",
        "path = './data/nowar_soldier/lmdb/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# victim style\n",
        "style = 'nowar_victim'\n",
        "path = './data/nowar_victim/lmdb/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = './checkpoint/'\n",
        "local_rank = 0\n",
        "size = 1024\n",
        "channel_multiplier = 2\n",
        "g_reg_every = 4\n",
        "d_reg_every = 16\n",
        "lr = 0.002\n",
        "ckpt_path = './checkpoint/stylegan2-ffhq-config-f.pt'\n",
        "iter = 400\n",
        "batch = 4\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu') # cuda 환경에서만 작동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GOF0bimTYwc"
      },
      "outputs": [],
      "source": [
        "# log 파일 저장할 폴더 만들기\n",
        "if not os.path.exists(\"log/%s/\"%(style)):\n",
        "    os.makedirs(\"log/%s/\"%(style))\n",
        "    print(\"created log folder\")\n",
        "# model ckpt파일 저장할 폴더 만들기 (checkpoint/stylename/)\n",
        "if not os.path.exists(\"%s/%s/\"%(model_path, style)):\n",
        "    os.makedirs(\"%s/%s/\"%(model_path, style))\n",
        "    print(\"created ckpt folder\")\n",
        "\n",
        "n_gpu = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1 # gpu 개수\n",
        "distributed = n_gpu > 1 # gpu가 여러 개일 경우, distributed\n",
        "\n",
        "if distributed:\n",
        "    torch.cuda.set_device(local_rank)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "    synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0fOMdUXWM3E"
      },
      "outputs": [],
      "source": [
        "latent = 512\n",
        "n_mlp = 8\n",
        "\n",
        "start_iter = 0\n",
        "\n",
        "#if arch == 'stylegan2':\n",
        "    #from model.stylegan.model import Generator, Discriminator\n",
        "\n",
        "#elif arch == 'swagan':\n",
        "    #from swagan import Generator, Discriminator\n",
        "\n",
        "\n",
        "# styleGAN2 불러오기\n",
        "generator = Generator(\n",
        "    size, latent, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "discriminator = Discriminator(\n",
        "    size, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "g_ema = Generator(\n",
        "    size, latent, n_mlp, channel_multiplier=channel_multiplier\n",
        ").to(device)\n",
        "g_ema.eval()\n",
        "accumulate(g_ema, generator, 0)\n",
        "\n",
        "g_reg_ratio = g_reg_every / (g_reg_every + 1)\n",
        "d_reg_ratio = d_reg_every / (d_reg_every + 1)\n",
        "\n",
        "g_optim = optim.Adam(\n",
        "    generator.parameters(),\n",
        "    lr=lr * g_reg_ratio,\n",
        "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
        ")\n",
        "d_optim = optim.Adam(\n",
        "    discriminator.parameters(),\n",
        "    lr=lr * d_reg_ratio,\n",
        "    betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3B4qN1WZgg",
        "outputId": "1266ed8a-a343-472e-f369-415a02d927a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load model: ./checkpoint/stylegan2-ffhq-config-f.pt\n"
          ]
        }
      ],
      "source": [
        "# ffhq 데이터로 pretrained된 stylegan 모델 불러오기\n",
        "if ckpt_path is not None:\n",
        "    print(\"load model:\", ckpt_path)\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    try:\n",
        "        ckpt_name = os.path.basename(ckpt_path)\n",
        "        start_iter = int(os.path.splitext(ckpt_name)[0])\n",
        "\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    generator.load_state_dict(ckpt[\"g\"])\n",
        "    discriminator.load_state_dict(ckpt[\"d\"])\n",
        "    g_ema.load_state_dict(ckpt[\"g_ema\"])\n",
        "\n",
        "    if \"g_optim\" in ckpt:\n",
        "        g_optim.load_state_dict(ckpt[\"g_optim\"])\n",
        "    if \"d_optim\" in ckpt:\n",
        "        d_optim.load_state_dict(ckpt[\"d_optim\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfikRZuqaiFK"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 준비\n",
        "if distributed:\n",
        "    generator = nn.parallel.DistributedDataParallel(\n",
        "        generator,\n",
        "        device_ids=[local_rank],\n",
        "        output_device=local_rank,\n",
        "        broadcast_buffers=False,\n",
        "    )\n",
        "\n",
        "    discriminator = nn.parallel.DistributedDataParallel(\n",
        "        discriminator,\n",
        "        device_ids=[local_rank],\n",
        "        output_device=local_rank,\n",
        "        broadcast_buffers=False,\n",
        "    )\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = MultiResolutionDataset(path, transform, size)\n",
        "loader = data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch,\n",
        "    sampler=data_sampler(dataset, shuffle=True, distributed=distributed),\n",
        "    drop_last=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "2V75h8imTHJR",
        "outputId": "e8f5011c-c8e3-4a36-a1ee-42658781c859"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meunai9\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/nowar/DualStyleGAN/wandb/run-20231113_105925-ifxsvobn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eunai9/stylegan%202/runs/ifxsvobn' target=\"_blank\">gentle-dream-11</a></strong> to <a href='https://wandb.ai/eunai9/stylegan%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/eunai9/stylegan%202' target=\"_blank\">https://wandb.ai/eunai9/stylegan%202</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/eunai9/stylegan%202/runs/ifxsvobn' target=\"_blank\">https://wandb.ai/eunai9/stylegan%202/runs/ifxsvobn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if get_rank() == 0 and wandb is not None and wandb:\n",
        "    wandb.init(project=\"stylegan 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K088DrPrZ4b0"
      },
      "source": [
        "## train 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y84D_BrJN4N8"
      },
      "outputs": [],
      "source": [
        "# !python -m torch.distributed.launch --nproc_per_node=1 --master_port=8765 finetune_stylegan.py --iter 600 \\\n",
        "# --batch 4 --ckpt ./checkpoint/stylegan2-ffhq-config-f.pt --style nowar --augment ./data/nowar/lmdb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X2xDCgBZ6vT"
      },
      "outputs": [],
      "source": [
        "augment_p = 0\n",
        "ada_target = 0.6; ada_length = 500 * 1000\n",
        "n_sample = 9\n",
        "mixing = 0.9\n",
        "path_batch_shrink = 2; path_regularize = 2\n",
        "save_every = 100\n",
        "r1 = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7vQRe-ZbHyQ",
        "outputId": "5f95feb8-7b95-4e01-e77f-83692d2cb1df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                                                               | 0/400 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "pbar = range(iter)\n",
        "\n",
        "if get_rank() == 0:\n",
        "    pbar = tqdm(pbar, initial=start_iter, ncols=140, dynamic_ncols=False, smoothing=0.01)\n",
        "\n",
        "mean_path_length = 0\n",
        "\n",
        "d_loss_val = 0\n",
        "r1_loss = torch.tensor(0.0, device=device)\n",
        "g_loss_val = 0\n",
        "path_loss = torch.tensor(0.0, device=device)\n",
        "path_lengths = torch.tensor(0.0, device=device)\n",
        "mean_path_length_avg = 0\n",
        "loss_dict = {}\n",
        "\n",
        "if distributed:\n",
        "    g_module = generator.module\n",
        "    d_module = discriminator.module\n",
        "\n",
        "else:\n",
        "    g_module = generator\n",
        "    d_module = discriminator\n",
        "\n",
        "accum = 0.5 ** (32 / (10 * 1000))\n",
        "ada_aug_p = augment_p if augment_p > 0 else 0.0\n",
        "r_t_stat = 0\n",
        "\n",
        "if augment and augment_p == 0:\n",
        "    ada_augment = AdaptiveAugment(ada_target, ada_length, 8, device)\n",
        "\n",
        "sample_z = torch.randn(n_sample, latent, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qplIEugwSD_6",
        "outputId": "fd076bc6-46f8-4f34-f48e-1a0087a35822"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nowar/DualStyleGAN/model/stylegan/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 2.1.0+cu118. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n",
            "iter: 00000; d: 2.9428; g: 1.1052; r1: 0.0143; path: 0.3087; mean path: 0.0056; augment: 0.0000:   0%|              | 0/400 [00:22<?, ?it/s]/content/drive/MyDrive/nowar/DualStyleGAN/model/stylegan/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 2.1.0+cu118. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n",
            "iter: 00399; d: 3.5694; g: 0.3300; r1: 0.0182; path: 0.0050; mean path: 0.1428; augment: 0.0026: 100%|████| 400/400 [27:03<00:00,  4.06s/it]\n"
          ]
        }
      ],
      "source": [
        "for idx in pbar:\n",
        "    i = idx + start_iter\n",
        "\n",
        "    if i > iter:\n",
        "        print(\"Done!\")\n",
        "\n",
        "        break\n",
        "\n",
        "    real_img = next(enumerate(loader))[1]\n",
        "    real_img = real_img.to(device)\n",
        "\n",
        "    requires_grad(generator, False)\n",
        "    requires_grad(discriminator, True)\n",
        "\n",
        "    noise = mixing_noise(batch, latent, mixing, device) #latent vector\n",
        "    fake_img, _ = generator(noise) #generator로 fake img 생성\n",
        "\n",
        "    if augment:\n",
        "        real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "        fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "    else:\n",
        "        real_img_aug = real_img\n",
        "\n",
        "    fake_pred = discriminator(fake_img) # fake img 판별\n",
        "    real_pred = discriminator(real_img_aug) # real img(nowar 데이터) 판별\n",
        "    d_loss = d_logistic_loss(real_pred, fake_pred) # loss 추출\n",
        "\n",
        "    loss_dict[\"d\"] = d_loss\n",
        "    loss_dict[\"real_score\"] = real_pred.mean()\n",
        "    loss_dict[\"fake_score\"] = fake_pred.mean()\n",
        "\n",
        "    discriminator.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_optim.step() #discriminator train\n",
        "\n",
        "    if augment and augment_p == 0:\n",
        "        ada_aug_p = ada_augment.tune(real_pred)\n",
        "        r_t_stat = ada_augment.r_t_stat\n",
        "\n",
        "    d_regularize = i % d_reg_every == 0\n",
        "\n",
        "    if d_regularize:\n",
        "        real_img.requires_grad = True\n",
        "\n",
        "        if augment:\n",
        "            real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "\n",
        "        else:\n",
        "            real_img_aug = real_img\n",
        "\n",
        "        real_pred = discriminator(real_img_aug)\n",
        "        r1_loss = d_r1_loss(real_pred, real_img)\n",
        "\n",
        "        discriminator.zero_grad()\n",
        "        (r1 / 2 * r1_loss * d_reg_every + 0 * real_pred[0]).backward()\n",
        "\n",
        "        d_optim.step()\n",
        "\n",
        "    loss_dict[\"r1\"] = r1_loss\n",
        "\n",
        "    requires_grad(generator, True)\n",
        "    requires_grad(discriminator, False)\n",
        "\n",
        "    noise = mixing_noise(batch, latent, mixing, device) # latent vector\n",
        "    fake_img, _ = generator(noise) # generator로 fake img 생성\n",
        "\n",
        "    if augment:\n",
        "        fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "    fake_pred = discriminator(fake_img) # fake img 판별\n",
        "    g_loss = g_nonsaturating_loss(fake_pred) # generator를 위한 loss\n",
        "\n",
        "    loss_dict[\"g\"] = g_loss\n",
        "\n",
        "    generator.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optim.step() # generator train\n",
        "\n",
        "    g_regularize = i % g_reg_every == 0\n",
        "\n",
        "    if g_regularize:\n",
        "        path_batch_size = max(1, batch // path_batch_shrink)\n",
        "        noise = mixing_noise(path_batch_size, latent, mixing, device)\n",
        "        fake_img, latents = generator(noise, return_latents=True)\n",
        "\n",
        "        path_loss, mean_path_length, path_lengths = g_path_regularize(\n",
        "            fake_img, latents, mean_path_length\n",
        "        )\n",
        "\n",
        "        generator.zero_grad()\n",
        "        weighted_path_loss = path_regularize * g_reg_every * path_loss\n",
        "\n",
        "        if path_batch_shrink:\n",
        "            weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n",
        "\n",
        "        weighted_path_loss.backward()\n",
        "\n",
        "        g_optim.step()\n",
        "\n",
        "        mean_path_length_avg = (\n",
        "            reduce_sum(mean_path_length).item() / get_world_size()\n",
        "        )\n",
        "\n",
        "    loss_dict[\"path\"] = path_loss\n",
        "    loss_dict[\"path_length\"] = path_lengths.mean()\n",
        "\n",
        "    accumulate(g_ema, g_module, accum)\n",
        "\n",
        "    loss_reduced = reduce_loss_dict(loss_dict)\n",
        "\n",
        "    d_loss_val = loss_reduced[\"d\"].mean().item()\n",
        "    g_loss_val = loss_reduced[\"g\"].mean().item()\n",
        "    r1_val = loss_reduced[\"r1\"].mean().item()\n",
        "    path_loss_val = loss_reduced[\"path\"].mean().item()\n",
        "    real_score_val = loss_reduced[\"real_score\"].mean().item()\n",
        "    fake_score_val = loss_reduced[\"fake_score\"].mean().item()\n",
        "    path_length_val = loss_reduced[\"path_length\"].mean().item()\n",
        "\n",
        "    if get_rank() == 0:\n",
        "        pbar.set_description(\n",
        "            (\n",
        "                f\"iter: {i:05d}; d: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; \"\n",
        "                f\"path: {path_loss_val:.4f}; mean path: {mean_path_length_avg:.4f}; \"\n",
        "                f\"augment: {ada_aug_p:.4f}\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if wandb and wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"Generator\": g_loss_val,\n",
        "                    \"Discriminator\": d_loss_val,\n",
        "                    \"Augment\": ada_aug_p,\n",
        "                    \"Rt\": r_t_stat,\n",
        "                    \"R1\": r1_val,\n",
        "                    \"Path Length Regularization\": path_loss_val,\n",
        "                    \"Mean Path Length\": mean_path_length,\n",
        "                    \"Real Score\": real_score_val,\n",
        "                    \"Fake Score\": fake_score_val,\n",
        "                    \"Path Length\": path_length_val,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if i % 100 == 0 or (i+1) == iter:\n",
        "            with torch.no_grad():\n",
        "                g_ema.eval()\n",
        "                sample, _ = g_ema([sample_z])\n",
        "                sample = F.interpolate(sample,256)\n",
        "                utils.save_image(\n",
        "                    sample,\n",
        "                    f\"log/%s/finetune-%06d.jpg\"%(style, i),\n",
        "                    nrow=int(n_sample ** 0.5),\n",
        "                    normalize=True,\n",
        "                    #range=(-1, 1),\n",
        "                )\n",
        "\n",
        "        if (i+1) % save_every == 0 or (i+1) == iter:\n",
        "            torch.save(\n",
        "                {\n",
        "                    #\"g\": g_module.state_dict(),\n",
        "                    #\"d\": d_module.state_dict(),\n",
        "                    \"g_ema\": g_ema.state_dict(),\n",
        "                    #\"g_optim\": g_optim.state_dict(),\n",
        "                    #\"d_optim\": d_optim.state_dict(),\n",
        "                    #\"args\": args,\n",
        "                    #\"ada_aug_p\": ada_aug_p,\n",
        "                },\n",
        "                f\"%s/%s/finetune-%06d.pt\"%(model_path, style, i+1),\n",
        "            )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
